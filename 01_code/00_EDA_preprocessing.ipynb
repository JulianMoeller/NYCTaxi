{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing raw bookingdata of NYC's Yellow Cab\n",
    "Using bookingdata of NYC Taxi and Limousine Commission provided via https://www.kaggle.com/c/nyc-taxi-trip-duration/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import libraries and define spacial and temporal aggregation level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h3\n",
    "from keplergl import KeplerGl\n",
    "from datetime import datetime, timedelta\n",
    "from meteostat import Point, Daily, Hourly\n",
    "\n",
    "h3_resolution = 7 # define hexagonal zone-size\n",
    "time_window = '1H' # define the timewindow length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explorative data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../00_data/00_raw_bookingdata/train.csv')\n",
    "df_test = pd.read_csv('../00_data/00_raw_bookingdata/test.csv')\n",
    "df_train['pickup_datetime'] =  pd.to_datetime(df_train['pickup_datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_test['pickup_datetime'] =  pd.to_datetime(df_test['pickup_datetime'], format='%Y-%m-%d %H:%M:%S')print(df_train['pickup_datetime'].min())\n",
    "\n",
    "print(df_train['pickup_datetime'].max())\n",
    "print(df_test['pickup_datetime'].min())\n",
    "print(df_test['pickup_datetime'].max())\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print(df_train.keys())\n",
    "print(df_test.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "- Dataset is not split in chronological subsets (not ordered by timestamps)\n",
    "    - Trainset contains 1.458.644 trips and 11 attributes between 1.1.16 and 30.6.16\n",
    "    - Testset contains 625.134 trips and 9 attributes between 1.1.16 and 30.6.16\n",
    "- Due to kaggle challange 'dropoff_datetime' and 'trip_duration' are missing in testdata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Approximate manhattan-area and convert coordinates to H3 id\n",
    "To allow for handling bookingdata as timeseries, training and testdata are being concatenated. Converting longitude and latitude into h3 index and saving it in a csv file to speedup following computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat dataframes\n",
    "df_trips = df_train.drop(['dropoff_datetime', 'trip_duration'], axis=1)\n",
    "df_trips = df_trips.append(df_test)\n",
    "df_trips.drop(['vendor_id', 'id', 'store_and_fwd_flag'], axis=1, inplace=True)\n",
    "df_trips = df_trips.sort_values(by='pickup_datetime')\n",
    "\n",
    "# limiting coordinates to approximate shape of manhattan\n",
    "long_range = [-73.903035, -74.025710]\n",
    "lat_range = [40.700372, 40.881485]\n",
    "\n",
    "df_trips = df_trips[\n",
    "    (df_trips.pickup_longitude <= long_range[0]) & \n",
    "    (df_trips.pickup_longitude >= long_range[1]) &\n",
    "    (df_trips.pickup_latitude >= lat_range[0]) &\n",
    "    (df_trips.pickup_latitude <= lat_range[1]) &\n",
    "    (df_trips.dropoff_longitude <= long_range[0]) & \n",
    "    (df_trips.dropoff_longitude >= long_range[1]) &\n",
    "    (df_trips.dropoff_latitude >= lat_range[0]) &\n",
    "    (df_trips.dropoff_latitude <= lat_range[1]) \n",
    "                   ]\n",
    "\n",
    "# augment data with H3 id\n",
    "h3_pickup_ids = [h3.geo_to_h3(lat=row['pickup_latitude'],lng=row['pickup_longitude'],resolution=h3_resolution) for index, row in df_trips.iterrows()]\n",
    "h3_dropoff_ids = [h3.geo_to_h3(lat=row['dropoff_latitude'],lng=row['dropoff_longitude'],resolution=h3_resolution) for index, row in df_trips.iterrows()]\n",
    "\n",
    "df_trips['pickup_h3'] = h3_pickup_ids\n",
    "df_trips['dropoff_h3'] = h3_dropoff_ids\n",
    "df_trips.to_csv('../00_data/01_cleaned_bookingdata/all_trips_h3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Spatial and temporal aggregation. Augement weatherdata\n",
    "\n",
    "Use preprocessed bookingdata and aggregate according to H3 id and timewindows. Due to missing dropoff_datetime in test-data only the pickup_datetime is used. Augment aggregated booking data with weatherinformation from https://dev.meteostat.net/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips = pd.read_csv('../00_data/01_cleaned_bookingdata/all_trips_h3.csv')\n",
    "df_trips['pickup_datetime'] =  pd.to_datetime(df_trips['pickup_datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "zone_ids = set(list(df_trips['pickup_h3'].values)+ list(df_trips['dropoff_h3'].values))\n",
    "\n",
    "# compute timewindows\n",
    "start = df_trips['pickup_datetime'].min().replace(minute=0, second=0, microsecond=0)\n",
    "end = df_trips['pickup_datetime'].max().replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "interval_df = pd.DataFrame({'start': [_ for _ in pd.date_range(start, end, freq=time_window)],})  \n",
    "interval_df['end'] = interval_df['start'].shift(-1)\n",
    "interval_df.dropna()\n",
    "interval_df['end'].iloc[-1] = (interval_df['start'].iloc[-1] + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "# get weatherdata\n",
    "manhattan = Point(40.754932, -73.984016, 2)\n",
    "weather_data = Hourly(manhattan, start, end).fetch() #  needs to be averaged over timewindow\n",
    "weather_attributes = ['temp', 'dwpt', 'rhum', 'prcp', 'snow', 'wdir', 'wspd', 'wpgt', 'pres', 'tsun', 'coco']\n",
    "\n",
    "# compute aggregated data\n",
    "columns = ['start', 'end'] + list(zone_ids) + weather_attributes\n",
    "df_zone_filter = pd.DataFrame(columns=columns)\n",
    "\n",
    "for index in range(interval_df.shape[0]):\n",
    "    row = interval_df.loc[index]\n",
    "    weather_entries = weather_data[row['start']:row['end']][:-1].mean(axis = 0, skipna = True).values.tolist()\n",
    "    event_filter = df_trips[(df_trips.pickup_datetime >= row['start']) & (df_trips.pickup_datetime <row['end'])]\n",
    "    counts = [event_filter[(event_filter.pickup_h3 == id)].shape[0] for id in zone_ids]\n",
    "    entry = pd.Series([row['start'], row['end']] + counts + weather_entries, index=df_zone_filter.columns)\n",
    "    df_zone_filter = df_zone_filter.append(entry, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train/Test/Validation split\n",
    "split into 80 % training data, 20% test data. Last 24 hours will be used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all datetimes\n",
    "start = df_zone_filter['start'].min().replace(minute=0, second=0, microsecond=0)\n",
    "end = df_zone_filter['start'].max().replace(minute=0, second=0, microsecond=0)\n",
    "date_range = [_ for _ in pd.date_range(start, end, freq='D')]\n",
    "\n",
    "# define dateranges of train/test/val\n",
    "train_dates = date_range[:146]\n",
    "test_dates = date_range[145:]\n",
    "validation_dates = date_range[-1:]\n",
    "\n",
    "# train/test/val split\n",
    "df_train_dummy = df_zone_filter.loc[\n",
    "    (df_zone_filter.start>=train_dates[0]) &\n",
    "    (df_zone_filter.start<train_dates[-1])\n",
    "]\n",
    "\n",
    "df_test_dummy = df_zone_filter.loc[\n",
    "    (df_zone_filter.start>=test_dates[0]) &\n",
    "    (df_zone_filter.start<test_dates[-1])\n",
    "]\n",
    "\n",
    "df_validation_dummy = df_zone_filter.loc[df_zone_filter.start>=validation_dates[0]]\n",
    "\n",
    "# export as csv\n",
    "df_train_dummy.to_csv('../00_data/02_processed_bookingdata/train.csv')\n",
    "df_test_dummy.to_csv('../00_data/02_processed_bookingdata/test.csv')\n",
    "df_validation_dummy.to_csv('../00_data/02_processed_bookingdata/validation.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
